apiVersion: v1
data:
  system.yaml: |
    secretNames:
      alibaba: kubeai-alibaba
      aws: kubeai-aws
      gcp: kubeai-gcp
      huggingface: huggingface
    resourceProfiles:
      amd-gpu-mi300x:
        imageName: amd-gpu
        limits:
          amd.com/gpu: "1"
        tolerations:
        - effect: NoSchedule
          key: amd.com/gpu
          operator: Equal
          value: present
      cpu:
        imageName: cpu
        requests:
          cpu: 1
          memory: 2Gi
      nvidia-gpu-a16:
        imageName: nvidia-gpu
        limits:
          nvidia.com/gpu: "1"
        tolerations:
        - effect: NoSchedule
          key: nvidia.com/gpu
          operator: Equal
          value: present
      nvidia-gpu-a100-40gb:
        imageName: nvidia-gpu
        limits:
          nvidia.com/gpu: "1"
        tolerations:
        - effect: NoSchedule
          key: nvidia.com/gpu
          operator: Equal
          value: present
      nvidia-gpu-a100-80gb:
        imageName: nvidia-gpu
        limits:
          nvidia.com/gpu: "1"
        tolerations:
        - effect: NoSchedule
          key: nvidia.com/gpu
          operator: Equal
          value: present
      nvidia-gpu-gh200:
        imageName: gh200
        limits:
          nvidia.com/gpu: "1"
        requests:
          nvidia.com/gpu: "1"
        tolerations:
        - effect: NoSchedule
          key: nvidia.com/gpu
          operator: Equal
          value: present
      nvidia-gpu-h100:
        imageName: nvidia-gpu
        limits:
          nvidia.com/gpu: "1"
        tolerations:
        - effect: NoSchedule
          key: nvidia.com/gpu
          operator: Equal
          value: present
      nvidia-gpu-l4:
        imageName: nvidia-gpu
        limits:
          nvidia.com/gpu: "1"
        requests:
          cpu: "6"
          memory: 24Gi
          nvidia.com/gpu: "1"
        tolerations:
        - effect: NoSchedule
          key: nvidia.com/gpu
          operator: Equal
          value: present
      nvidia-gpu-l40s:
        imageName: nvidia-gpu
        limits:
          nvidia.com/gpu: "1"
        requests:
          cpu: "6"
          memory: 24Gi
          nvidia.com/gpu: "1"
        tolerations:
        - effect: NoSchedule
          key: nvidia.com/gpu
          operator: Equal
          value: present
      nvidia-gpu-rtx4070-8gb:
        imageName: nvidia-gpu
        limits:
          nvidia.com/gpu: "1"
        tolerations:
        - effect: NoSchedule
          key: nvidia.com/gpu
          operator: Equal
          value: present
      nvidia-gpu-t4:
        imageName: nvidia-gpu
        limits:
          nvidia.com/gpu: "1"
        requests:
          nvidia.com/gpu: "1"
    cacheProfiles:
      {}
    modelServers:
      FasterWhisper:
        images:
          default: fedirz/faster-whisper-server:latest-cpu
          nvidia-gpu: fedirz/faster-whisper-server:latest-cuda
      Infinity:
        images:
          amd-gpu: michaelf34/infinity:0.0.77-rocm
          cpu: michaelf34/infinity:0.0.77-cpu
          default: michaelf34/infinity:0.0.77
          nvidia-gpu: michaelf34/infinity:0.0.77
      OLlama:
        images:
          amd-gpu: ollama/ollama:0.11.11-rocm
          default: ollama/ollama:0.11.11
          nvidia-gpu: ollama/ollama:0.11.11
      VLLM:
        images:
          amd-gpu: rocm/vllm/rocm7.0.0_vllm_0.11.1_20251103
          cpu: public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo:v0.11.2
          default: vllm/vllm-openai:v0.11.2
          gh200: vllm/vllm-openai:v0.11.2
          google-tpu: vllm/vllm-tpu:23194d83e8f2a6783b0d8c275f5f8a22faab9aec
          nvidia-gpu: vllm/vllm-openai:v0.11.2
    modelLoading:
      image: ghcr.io/kubeai-project/kubeai-model-loader:v0.14.0
    modelRollouts:
      surge: 1
    modelServerPods:
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
      serviceAccountName: kubeai-models
    modelAutoscaling:
      interval: 10s
      timeWindow: 10m
      stateConfigMapName: kubeai-autoscaler-state
    messaging:
      errorMaxBackoff: 30s
      streams: []
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: kubeai
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeai
    app.kubernetes.io/version: v0.23.1
    helm.sh/chart: kubeai-0.23.1
  name: kubeai-config
  namespace: kubeai
